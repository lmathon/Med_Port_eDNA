dplyr::last(names(sim_features_depths))
)
) %>%
dplyr::transmute(
id1 = id,
id2 = id,
boundary = edge_length
)
)
### aggregate data so we don't have any duplicate rows with exactly
### the same id1 and id2 values
bound_data <-
bound_data %>%
group_by(id1, id2) %>%
summarize(boundary = sum(boundary)) %>%
ungroup()
# now let's create the problem
p <-
problem(pu_data$cost, features = feat_data, rij_matrix = rij_data) %>%
add_min_set_objective() %>%
add_relative_targets(0.1) %>%
add_boundary_penalties(50,0.5,data=bound_data) %>%
#add_contiguity_constraints()%>%
add_binary_decisions()
# generate a solution
s <- solve(p)
# now let's create the problem
p <-
problem(pu_data$cost, features = feat_data, rij_matrix = rij_data) %>%
add_min_set_objective() %>%
add_relative_targets(0.1) %>%
add_boundary_penalties(50,0.5,data=bound_data) %>%
add_contiguity_constraints()%>%
add_binary_decisions()
library(prioritizrdata)
library(prioritizr)
library(dplyr)
library(tibble)
library(scales)
library(ggplot2)
library(topsis)
library(withr)
install.packages("prioritizrdata")
install.packages("topsis")
library(prioritizrdata)
library(topsis)
# load planning unit data
data(tas_pu)
# convert planning units to sf format
tas_pu <- st_as_sf(tas_pu)
# load feature data
data(tas_features)
print(tas_pu)
print(tas_features)
p0 <- problem(tas_pu, tas_features, cost_column = "cost") %>%
add_min_set_objective() %>%
add_relative_targets(0.17) %>%
add_locked_in_constraints("locked_in") %>%
add_binary_decisions()
s0 <- solve(p0)
p1 <- problem(tas_pu, tas_features, cost_column = "cost") %>%
add_min_set_objective() %>%
add_relative_targets(0.17) %>%
add_locked_in_constraints("locked_in") %>%
add_binary_decisions() %>%
add_default_solver(gap = 0)
# solve problem
s1 <- solve(p1)
View(s1)
s1_cost <- eval_cost_summary(p1, s1[, "solution_1"])$cost
total_boundary_length = eval_boundary_summary(p1, s1)$boundary
total_boundary_length = eval_boundary_summary(p1, s1[, "solution_1"])$boundary
load("C:/Users/mathon/Desktop/linux/Pole2Pole_eDNA/Rdata/03-filter-data.Rdata")
library(tidyverse)
station <- df_filtered %>% distinct(station, .keep_all=T)
station <- select(station, latitude_start, longitude_start, date)
View(station)
station <- df_filtered %>% distinct(station, .keep_all=T)
View(station)
station <- select(station, latitude_start, longitude_start, date)
station <- df_filtered %>% distinct(station, .keep_all=T)
station <- station %>% select(station, latitude_start, longitude_start, date)
write.csv(station, file="c://Users/mathon/Desktop/eDNA_data.csv", row.names = F)
save(station, file="c://Users/mathon/Desktop/eDNA_data.rdata")
load("C:/Users/mathon/Desktop/linux/Pole2Pole_eDNA/Rdata/02-clean-data.Rdata")
load("C:/Users/mathon/Desktop/linux/Pole2Pole_eDNA/Rdata/03-filter-data.Rdata")
eDNA_filters <- df_all_filters %>% distinct(sample_name_all_pcr, .keep_all=T)
library(tidyverse)
eDNA_filters <- df_all_filters %>% distinct(sample_name_all_pcr, .keep_all=T)
View(eDNA_filters)
eDNA_filters <- eDNA_filters[,c(7:9, 22:46)]
eDNA_filters <- df_all_filters %>% distinct(sample_name_all_pcr, .keep_all=T)
eDNA_filters <- eDNA_filters[,c(9, 22:46, 7:8)]
write.csv(eDNA_filters, file="c://Users/mathon/Desktop/eDNA_filters_metadata.csv", row.names = F)
write.csv(df_all_filters, file="c://Users/mathon/Desktop/eDNA_filters_MOTUs.csv", row.names = F)
load("C:/Users/mathon/Desktop/linux/Seamounts_3Dmodelling/00_metadata/edna_explanatory_variables_benthic.rdata")
View(edna_var)
load("C:/Users/mathon/Desktop/linux/Seamounts_3Dmodelling/00_metadata/bruvs_explanatory_variables.rdata")
View(bruvs_var)
View(edna_var)
load("C:/Users/mathon/Desktop/linux/Seamounts_3Dmodelling/04_Modelling/02_pelagic/01_acoustic/BRT_Output_acoustic/pelagic_acoustic_predict.rdata")
View(pelagic_acoustic_predict)
load("C:/Users/mathon/Desktop/linux/Seamounts_3Dmodelling/04_Modelling/02_pelagic/02_eDNA/BRT_Output_edna/pelagic_motu_predict.rdata")
View(pelagic_motu_predict)
load("C:/Users/mathon/Desktop/linux/Seamounts_3Dmodelling/05_Marine_Spatial_Planning/03_Prioritization/Rdata/hierarchical_results.rdata")
View(hierarchical_results)
library(devtools)
load("C:/Users/mathon/Desktop/linux/Seamounts_3Dmodelling/02_formating_data/01_Benthic/Rdata/edna_richness_benthic.rdata")
load("C:/Users/mathon/Desktop/linux/Seamounts_3Dmodelling/02_formating_data/01_Benthic/Rdata/bruvs_richness_all.rdata")
ZEE <- st_read("c://Users/mathon/Downloads/World_EEZ_v11_20191118/eez_boundaries_v11.shp")
library(raster)
library(tidyverse)
library(sf)
library(rgeos)
library(ggplot2)
ZEE <- st_read("c://Users/mathon/Downloads/World_EEZ_v11_20191118/eez_boundaries_v11.shp")
plot(ZEE)
View(ZEE)
plot(ZEE[,"TERRITORY1"=="New Caledonia"])
ZEE_NC <- ZEE %>%
filter(TERRITORY1=="New Caledonia")
plot(ZEE_NC$TERRITORY1)
plot(ZEE_NC)
plot(ZEE_NC[3])
ZEE_NC <- ZEE %>%
filter(TERRITORY1=="New Caledonia") %>%
filter(LINE_TYPE!="Straight Baseline")
plot(ZEE_NC[3])
plot(ZEE_NC)
ZEE_NC <- ZEE %>%
filter(TERRITORY1=="New Caledonia")
plot(ZEE_NC)
ZEE_NC <- ZEE %>%
filter(TERRITORY1=="New Caledonia") %>%
filter(LINE_TYPE!="Straight Baseline")
plot(ZEE_NC[6])
ZEE_contour <- ZEE_NC[6]
plot(ZEE_contour)
world <- st_read("c://Users/mathon/Desktop/PhD/Projets/Megafauna/Carto_megafauna/GSHHS_f_L1.shp")
ggplot()+
geom_sf(aes(), data = world, fill = "grey80", col="black") +
geom_sf(aes(), data=ZEE_contour, fill=NA, col="red", size=2)+
coord_sf(xlim = c(113, 173), ylim = c(-40, -10))+
#geom_rect(aes(xmin = 157, xmax = 170, ymin = -24.2, ymax = -19.25), color = "red", fill = NA)+
guides(col=FALSE)+
theme_minimal()+
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_rect(colour = "black", size=1, fill=NA))
ggplot()+
geom_sf(aes(), data = world, fill = "grey80", col="black") +
geom_sf(aes(), data=ZEE_contour, fill=NA, col="red", size=2)+
coord_sf(xlim = c(113, 173), ylim = c(-40, -10))+
guides(col=FALSE)+
theme_minimal()+
theme(axis.text.x = element_blank(),
axis.text.y = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_rect(colour = "black", size=1, fill=NA))
load("C:/Users/mathon/Desktop/linux/Seamounts_3Dmodelling/01_Raw_data/Clean_eDNA/Rdata/02-clean-data.Rdata")
library(tidyverse)
n_distinct(df_all_filters$run)
load("C:/Users/mathon/Desktop/linux/Seamounts_3Dmodelling/05_Marine_Spatial_Planning/02_formating_MSP3D_inputs/Rdata/features_depth.rdata")
View(features_depth)
load("C:/Users/mathon/Desktop/linux/Seamounts_3Dmodelling/05_Marine_Spatial_Planning/02_formating_MSP3D_inputs/Rdata/pu_data.rdata")
View(pu_data)
View(features_depth)
load("C:/Users/mathon/Desktop/linux/Seamounts_3Dmodelling/05_Marine_Spatial_Planning/02_formating_MSP3D_inputs/Rdata/rij_data.rdata")
View(rij_data)
load("C:/Users/mathon/Desktop/linux/Seamounts_3Dmodelling/05_Marine_Spatial_Planning/02_formating_MSP3D_inputs/Rdata/feat_data.rdata")
View(feat_data)
load("C:/Users/mathon/Desktop/linux/Pole2Pole_eDNA/Rdata/02-clean-data.Rdata")
View(df_all_filters)
library(usethis)
library(devtools)
install_version("ade4", version = "1.7-17", repos = "https://cran.r-project.org/src/contrib/Archive/ade4/ade4_1.7-17.tar.gz")
install_version("ade4", version = "1.7-17")
library("ade4", lib.loc="~/R/R-4.1.1/library")
detach("package:ade4", unload=TRUE)
# Definition of work directories
repertoire <- "c:///Users/mathon/Desktop/Elding" # GIT HUB
# identification of sub-folders
repertoire.input <- paste0(repertoire,"/INPUT")
repertoire.output <- paste0(repertoire,"/OUTPUT")
repertoire.script <- paste0(repertoire,"/SCRIPT")
# Loading the packages needed for the script
library(googlesheets4)
library(dplyr)
library(ggplot2)
library(tidyr)
# Loading of home-made functions
source(paste0(repertoire.script,"/1_Cleaning_DATA_2021_humpbacks.R"))
DATA_2021_humpbacks <- read.csv(paste0(repertoire.input,"/DATA_2021_humpbacks.csv"), sep=";")
# 1 : Cleaning and homogenization of DATA_2021_humpbacks ---------------------------------------------------------------
DATA_2021_humpbacks_MAJ <- Cleaning_DATA_2021_humpbacks(DATA_2021_humpbacks)
library(tidyverse)
# 1 : Cleaning and homogenization of DATA_2021_humpbacks ---------------------------------------------------------------
DATA_2021_humpbacks_MAJ <- Cleaning_DATA_2021_humpbacks(DATA_2021_humpbacks)
View(DATA_2021_humpbacks)
# 1.1 Delete all columns that are completely empty
Ncol <- c()
for(i in 1:dim(DATA_2021_humpbacks)[2]){
if( length(c(unique(is.na(DATA_2021_humpbacks[,i])[,1]))) == 1 &
unique(is.na(DATA_2021_humpbacks[,i])[,1])==TRUE ){Ncol[i] <- i}
}
is.na(DATA_2021_humpbacks[,i])
is.na(DATA_2021_humpbacks[,i])
unique(is.na(DATA_2021_humpbacks[,i]))
c(unique(is.na(DATA_2021_humpbacks[,i])))
length(c(unique(is.na(DATA_2021_humpbacks[,i]))))
DATA_2021_humpbacks[,i][,1]
is.na(DATA_2021_humpbacks[,i])
unique(is.na(DATA_2021_humpbacks[,i])[,1])
# 1.1 Delete all columns that are completely empty
Ncol <- c()
for(i in 1:dim(DATA_2021_humpbacks)[2]){
if( length(c(unique(is.na(DATA_2021_humpbacks[,i])))) == 1 &
unique(is.na(DATA_2021_humpbacks[,i]))==TRUE ){Ncol[i] <- i}
}
Ncol
as.data.frame(unique(Ncol)[2:length(unique(Ncol))])
# To have the list of the lines of the if
deleted_Col <- as.data.frame(unique(Ncol)[2:length(unique(Ncol))])
# pour suprimer par block de 4 et ??tre ertain de conserver les bonne col de NA si besoin ( si juste une info / 4)
deleted_Col$Verif <- NA
for(j in seq(14,dim(DATA_2021_humpbacks)[2], by= 4)){
for ( i in 1:dim(deleted_Col)[1]){
Vec <- c(deleted_Col[i,1],deleted_Col[i+1,1],deleted_Col[i+2,1],deleted_Col[i+3,1])
if(Vec%in%j:(j+3) & length(unique(Vec%in%j:(j+3)))==1 & unique(Vec%in% j:(j+3))==TRUE){
if( is.na(deleted_Col[i,2])){ deleted_Col[i:(i+3),2] <- "OUI"}  # pour le moment dit oui que sur 1
}
}
}
deleted_Col <- filter(deleted_Col, Verif=="OUI")
View(deleted_Col)
# Loading of home-made functions
source(paste0(repertoire.script,"/1_Cleaning_DATA_2021_humpbacks.R"))
# Definition of work directories
repertoire <- "c:///Users/mathon/Desktop/Elding" # GIT HUB
# identification of sub-folders
repertoire.input <- paste0(repertoire,"/INPUT")
repertoire.output <- paste0(repertoire,"/OUTPUT")
repertoire.script <- paste0(repertoire,"/SCRIPT")
# Loading the packages needed for the script
library(googlesheets4)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(tidyr)
# Loading of home-made functions
source(paste0(repertoire.script,"/1_Cleaning_DATA_2021_humpbacks.R"))
DATA_2021_humpbacks <- read.csv(paste0(repertoire.input,"/DATA_2021_humpbacks.csv"), sep=";")
# 1 : Cleaning and homogenization of DATA_2021_humpbacks ---------------------------------------------------------------
DATA_2021_humpbacks_MAJ <- Cleaning_DATA_2021_humpbacks(DATA_2021_humpbacks)
# To clean the empty col in the data and save celles qui vont par group de 4 (date / location / lat / long )
DATA_2021_humpbacks_Clean_COL <- DATA_2021_humpbacks[,-as.vector(deleted_Col[,1])]
# 1.1 Delete all columns that are completely empty
Ncol <- c()
for(i in 1:dim(DATA_2021_humpbacks)[2]){
if( length(c(unique(is.na(DATA_2021_humpbacks[,i])))) == 1 &
unique(is.na(DATA_2021_humpbacks[,i]))==TRUE ){Ncol[i] <- i}
}
# To have the list of the lines of the if
deleted_Col <- as.data.frame(unique(Ncol)[2:length(unique(Ncol))])
# pour suprimer par block de 4 et ??tre ertain de conserver les bonne col de NA si besoin ( si juste une info / 4)
deleted_Col$Verif <- NA
for(j in seq(14,dim(DATA_2021_humpbacks)[2], by= 4)){
for ( i in 1:dim(deleted_Col)[1]){
Vec <- c(deleted_Col[i,1],deleted_Col[i+1,1],deleted_Col[i+2,1],deleted_Col[i+3,1])
if(Vec%in%j:(j+3) & length(unique(Vec%in%j:(j+3)))==1 & unique(Vec%in% j:(j+3))==TRUE){
if( is.na(deleted_Col[i,2])){ deleted_Col[i:(i+3),2] <- "OUI"}  # pour le moment dit oui que sur 1
}
}
}
deleted_Col <- filter(deleted_Col, Verif=="OUI")
# To clean the empty col in the data and save celles qui vont par group de 4 (date / location / lat / long )
DATA_2021_humpbacks_Clean_COL <- DATA_2021_humpbacks[,-as.vector(deleted_Col[,1])]
View(DATA_2021_humpbacks_Clean_COL)
# 1.1.2 Delete all row that are completely empty (one the 350)
DATA_2021_humpbacks_Clean_COL <- DATA_2021_humpbacks_Clean_COL[-350,]
# Couper le data en deux et multiplier au besoin les lignes du permeir pour avoir une aucurence de chaque ligne du second
DATA_2021_humpbacks_1 <- DATA_2021_humpbacks_Clean_COL[,1:13]
# Cree un data des obs et rajouter (une col nb obsTT) dans data 1 qui permettera de savoir d combien on dois multiplier les lignes du data 1 pour mettre les dates / Site / lat / log
DATA_2021_humpbacks_2 <- DATA_2021_humpbacks_Clean_COL[,c(1,14:dim(DATA_2021_humpbacks_Clean_COL)[2])]
# Recuprerer selement les dates d'observation pour les compter
DATA_2021_humpbacks_2.1 <- DATA_2021_humpbacks_2[,seq(2,dim(DATA_2021_humpbacks_2)[2], by= 4)]
# Comptage
DATA_2021_humpbacks_2$Nb_obs_Total <- NA
for ( i in 1: dim(DATA_2021_humpbacks_2)[1]){
DATA_2021_humpbacks_2[i,"Nb_obs_Total"] <- length(unique(unname(unlist(DATA_2021_humpbacks_2.1[i,]))))-1
}
# Pour visualiser les id et le nombre dobs tt
DATA_2021_humpbacks_2.2 <- DATA_2021_humpbacks_2[,c(1,dim(DATA_2021_humpbacks_2)[2])]
# regarder si il n'y a pas de doublons
length(unique(unname(unlist(DATA_2021_humpbacks_2.2[,1]))))==dim(DATA_2021_humpbacks_2.2)[1] #
# /!\/!\/!\/!\/!\/!\/!\/!\/!\/!\/!\   un ID NA   /!\/!\/!\/!\/!\/!\/!\/!\/!\/!\/!\/!\/!\/!\/!\
# Atribution nom a l'id NA
DATA_2021_humpbacks_2.2[349,"ID"] <- "NO_ID"
DATA_2021_humpbacks_1[349,"ID"] <- "NO_ID"
# Pour int??grer les nb obs tt au data 1
DATA_2021_humpbacks_1$Nb_obs_Total <- NA
for(i in 1:dim(DATA_2021_humpbacks_1)[1]){
if(is.na(DATA_2021_humpbacks_1[i,"ID"])!=TRUE){
DATA_2021_humpbacks_1[i,"Nb_obs_Total"] <- DATA_2021_humpbacks_2.2[ DATA_2021_humpbacks_2.2$ID==as.character(DATA_2021_humpbacks_2.2[i,"ID"]) ,"Nb_obs_Total"][1,1]
}
if(is.na(DATA_2021_humpbacks_1[i,"ID"])==TRUE){  DATA_2021_humpbacks_1[i,"Nb_obs_Total"] <-DATA_2021_humpbacks_2.2[349,"Nb_obs_Total"] }
}
i=1
DATA_2021_humpbacks_2.2[ DATA_2021_humpbacks_2.2$ID==as.character(DATA_2021_humpbacks_2.2[i,"ID"]) ,"Nb_obs_Total"][1,1]
as.character(DATA_2021_humpbacks_2.2[i,"ID"])
DATA_2021_humpbacks_2.2$ID==as.character(DATA_2021_humpbacks_2.2[i,"ID"]) ,"Nb_obs_Total"
[ DATA_2021_humpbacks_2.2$ID==as.character(DATA_2021_humpbacks_2.2[i,"ID"]) ,"Nb_obs_Total"]
is.na(DATA_2021_humpbacks_1[i,"ID"])
DATA_2021_humpbacks_2.2[349,"Nb_obs_Total"]
# Pour int??grer les nb obs tt au data 1
DATA_2021_humpbacks_1$Nb_obs_Total <- NA
for(i in 1:dim(DATA_2021_humpbacks_1)[1]){
if(is.na(DATA_2021_humpbacks_1[i,"ID"])!=TRUE){
DATA_2021_humpbacks_1[i,"Nb_obs_Total"] <- DATA_2021_humpbacks_2.2[i, "Nb_obs_Total"]
}
if(is.na(DATA_2021_humpbacks_1[i,"ID"])==TRUE){  DATA_2021_humpbacks_1[i,"Nb_obs_Total"] <-DATA_2021_humpbacks_2.2[349,"Nb_obs_Total"] }
}
DATA_2021_humpbacks_1$Nb_obs_Total
View(DATA_2021_humpbacks_2.1)
.libpaths()
.libPaths()
# Definition of work directories
repertoire <- "c:///Users/mathon/Desktop/Elding" # GIT HUB
# identification of sub-folders
repertoire.input <- paste0(repertoire,"/INPUT")
repertoire.output <- paste0(repertoire,"/OUTPUT")
repertoire.script <- paste0(repertoire,"/SCRIPT")
# Loading the packages needed for the script
library(googlesheets4)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(tidyr)
# Loading of home-made functions
source(paste0(repertoire.script,"/1_Cleaning_DATA_2021_humpbacks.R"))
DATA_2021_humpbacks <- read.csv(paste0(repertoire.input,"/DATA_2021_humpbacks.csv"), sep=";")
# 1 : Cleaning and homogenization of DATA_2021_humpbacks ---------------------------------------------------------------
DATA_2021_humpbacks_MAJ <- Cleaning_DATA_2021_humpbacks(DATA_2021_humpbacks)
# 1.1 Delete all columns that are completely empty
Ncol <- c()
for(i in 1:dim(DATA_2021_humpbacks)[2]){
if( length(c(unique(is.na(DATA_2021_humpbacks[,i])))) == 1 &
unique(is.na(DATA_2021_humpbacks[,i]))==TRUE ){Ncol[i] <- i}
}
# To have the list of the lines of the if
deleted_Col <- as.data.frame(unique(Ncol)[2:length(unique(Ncol))])
# pour suprimer par block de 4 et ??tre ertain de conserver les bonne col de NA si besoin ( si juste une info / 4)
deleted_Col$Verif <- NA
for(j in seq(14,dim(DATA_2021_humpbacks)[2], by= 4)){
for ( i in 1:dim(deleted_Col)[1]){
Vec <- c(deleted_Col[i,1],deleted_Col[i+1,1],deleted_Col[i+2,1],deleted_Col[i+3,1])
if(Vec%in%j:(j+3) & length(unique(Vec%in%j:(j+3)))==1 & unique(Vec%in% j:(j+3))==TRUE){
if( is.na(deleted_Col[i,2])){ deleted_Col[i:(i+3),2] <- "OUI"}  # pour le moment dit oui que sur 1
}
}
}
deleted_Col <- filter(deleted_Col, Verif=="OUI")
# To clean the empty col in the data and save celles qui vont par group de 4 (date / location / lat / long )
DATA_2021_humpbacks_Clean_COL <- DATA_2021_humpbacks[,-as.vector(deleted_Col[,1])]
# 1.1.2 Delete all row that are completely empty (one the 350)
DATA_2021_humpbacks_Clean_COL <- DATA_2021_humpbacks_Clean_COL[-350,]
# Couper le data en deux et multiplier au besoin les lignes du permeir pour avoir une aucurence de chaque ligne du second
DATA_2021_humpbacks_1 <- DATA_2021_humpbacks_Clean_COL[,1:13]
# Cree un data des obs et rajouter (une col nb obsTT) dans data 1 qui permettera de savoir d combien on dois multiplier les lignes du data 1 pour mettre les dates / Site / lat / log
DATA_2021_humpbacks_2 <- DATA_2021_humpbacks_Clean_COL[,c(1,14:dim(DATA_2021_humpbacks_Clean_COL)[2])]
# Recuprerer selement les dates d'observation pour les compter
DATA_2021_humpbacks_2.1 <- DATA_2021_humpbacks_2[,seq(2,dim(DATA_2021_humpbacks_2)[2], by= 4)]
View(DATA_2021_humpbacks_2.1)
# Comptage
DATA_2021_humpbacks_2$Nb_obs_Total <- NA
for ( i in 1: dim(DATA_2021_humpbacks_2)[1]){
DATA_2021_humpbacks_2[i,"Nb_obs_Total"] <- length(unique(unname(unlist(DATA_2021_humpbacks_2.1[i,]))))-1
}
View(DATA_2021_humpbacks_2)
# Pour visualiser les id et le nombre dobs tt
DATA_2021_humpbacks_2.2 <- DATA_2021_humpbacks_2[,c(1,dim(DATA_2021_humpbacks_2)[2])]
View(DATA_2021_humpbacks_2.2)
View(DATA_2021_humpbacks)
#
Akureyri_site_fidelity <- read_sheet("https://docs.google.com/spreadsheets/d/1VwSjVyAHDD8tGULjDH1LK4Qar2jRaGbmtOPtpSMY5nY/edit?usp=sharing_eip_m&ts=635810e7")
library(tidyverse)
library(sp)
library(rgeos)
library(rgdal)
library(maptools)
library(gissr)
library(raster)
a <- c(rep("A", 3), rep("B", 3), rep("C",2))
b <- c('A','B','B','C','A','A','B','B')
df <-data.frame(a,b)
View(df)
df = df[!duplicated(df),]
df
for (i in 1:nrow(df))
{
df[i, ] = sort(df[i, ])
}
for (i in 1:nrow(df)){
df[i, ] = sort(df[i, ])
}
df
a <- c(rep("A", 3), rep("B", 3), rep("C",2))
b <- c('A','B','B','C','A','A','B','B')
df <-data.frame(a,b)
for (i in 1:nrow(df)){
df[i, ] = sort(df[i, ])
}
df
df = df[!duplicated(df),]
df
setwd("C:/Users/mathon/Desktop/linux/Med_Port_eDNA")
library(dplyr)
library(forcats)
library(stringr)
library(rsq)
library(margins)
library(betapart)
library(reshape)
library(tidyverse)
library(tidyselect)
library(vegan)
library(ggplot2)
library(patchwork)
library(ggalt)
library(ggrepel)
library(grid)
library(ggpubr)
library(RColorBrewer)
library(rdacca.hp)
###########################################################################################
## dbRDA ports only
###########################################################################################
biodiv_port=read.csv("01_Analyses_teleo/00_data/matrice_teleo_port.csv", row.names=1) %>%
t(.) %>%
as.data.frame(.) %>%
rownames_to_column(var='code_spygen')
# metadata
meta_port=read.csv("00_Metadata/metadata_port.csv", sep=";")
# combiner toutes les donnees
data_port=left_join(biodiv_port,meta_port)
rownames(data_port)=data_port[,"code_spygen"]
# remove biohut
data_port = data_port %>%
filter(type == "Port")
#### faire la dbRDA
data_dbrda_port <- data_port[,c(2:123)]
meta_dbrda_port <- data_port[,c(124:ncol(data_port))]
View(meta_port)
setnames(meta_dbrda_port,
old = c('Campaign'),
new = c('Season'))
library(data.table)
setnames(meta_dbrda_port,
old = c('Campaign'),
new = c('Season'))
View(meta_port)
meta_dbrda_port <- setnames(meta_dbrda_port,
old = c('Campaign'),
new = c('Season'))
meta_dbrda_port <- gsub(names(meta_dbrda_port), "Campaign","Season")
meta_dbrda_port <- data_port[,c(124:ncol(data_port))]
names(meta_dbrda_port) <- gsub(names(meta_dbrda_port), "Campaign","Season")
View(meta_port)
colnames(meta_dbrda_port)[colnames(meta_dbrda_port) == "Campaign"] <- "Season"
View(meta_port)
View(meta_dbrda_port)
meta_dbrda_port <- data_port[,c(124:ncol(data_port))]
colnames(meta_dbrda_port)[colnames(meta_dbrda_port) == "Campaign"] <- "Season"
View(meta_dbrda_port)
rdacca.hp(vegdist(data_dbrda_port,method="jaccard"),
meta_dbrda_port[,c("Season","Certification","Area_ha","Depth_m","Habitat", "Longitude")],method="dbRDA",add=F,type="R2")
rda.port<-rdacca.hp(vegdist(data_dbrda_port,method="jaccard"),
meta_dbrda_port[,c("Season","Certification","Area_ha","Depth_m","Habitat", "Longitude")],method="dbRDA",add=F,type="R2")
rda.port
plot(rda.port)
ggsave(file="01_Analyses_teleo/03_Outputs/rdacca_port_FigureSXX.png")
rda_port<- as.data.frame(rda.port$Hier.part)
write.csv(rda_port, file="01_Analyses_teleo/03_Outputs/Table_SXX_rdacca_port.csv", row.names = F)
###########################################################################################
## dbRDA ports only
###########################################################################################
biodiv_port=read.csv("01_Analyses_teleo/00_data/reduced_matrice_teleo_port.csv", row.names=1) %>%
t(.) %>%
as.data.frame(.) %>%
rownames_to_column(var='code_spygen')
# metadata
meta_port=read.csv("00_Metadata/metadata_port.csv", sep=";")
# combiner toutes les donnees
data_port=left_join(biodiv_port,meta_port)
rownames(data_port)=data_port[,"code_spygen"]
# remove biohut
data_port = data_port %>%
filter(type == "Port")
#### faire la dbRDA
data_dbrda_port <- data_port[,c(2:102)]
meta_dbrda_port <- data_port[,c(102:ncol(data_port))]
colnames(meta_dbrda_port)[colnames(meta_dbrda_port) == "Campaign"] <- "Season"
rdacca.hp(vegdist(data_dbrda_port,method="jaccard"),
meta_dbrda_port[,c("Season","Certification","Area_ha","Depth_m","Habitat", "Longitude")],method="dbRDA",add=F,type="R2")
rda.port<-rdacca.hp(vegdist(data_dbrda_port,method="jaccard"),
meta_dbrda_port[,c("Season","Certification","Area_ha","Depth_m","Habitat", "Longitude")],method="dbRDA",add=F,type="R2")
rda.port
plot(rda.port)
ggsave(file="01_Analyses_teleo/02_Analyses/Analyses_reduced_species_list/Outputs/rdacca_port_FigureSXX.png")
rda_port<- as.data.frame(rda.port$Hier.part)
write.csv(rda_port, file="01_Analyses_teleo/02_Analyses/Analyses_reduced_species_list/Outputs/Table_SXX_rdacca_port.csv", row.names = F)
